{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca46273c-6f56-4d33-8e8f-ebd64522586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import glob, os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e80db2-c65c-4179-8cf5-db7b454e72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('AB.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('Unziped AB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2409bfdf-29f5-429e-841e-18329d20106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "#  Dataset (Unpaired)\n",
    "# =======================\n",
    "class UnpairedDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, \"A\") + \"/*.*\"))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, \"B\") + \"/*.*\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_A = Image.open(self.files_A[idx % len(self.files_A)]).convert(\"RGB\")\n",
    "        img_B = Image.open(self.files_B[random.randint(0, len(self.files_B)-1)]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img_A = self.transform(img_A)\n",
    "            img_B = self.transform(img_B)\n",
    "        return img_A, img_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecee748b-8ed3-494e-9081-95a79508ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "#  Generator (U-Net)\n",
    "# =======================\n",
    "def conv_block(in_c, out_c, norm=True):\n",
    "    layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "    if norm: layers.append(nn.InstanceNorm2d(out_c))\n",
    "    layers.append(nn.LeakyReLU(0.2, True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def deconv_block(in_c, out_c, dropout=False):\n",
    "    layers = [\n",
    "        nn.ConvTranspose2d(in_c, out_c, 4, 2, 1),\n",
    "        nn.InstanceNorm2d(out_c),\n",
    "        nn.ReLU(True)\n",
    "    ]\n",
    "    if dropout: layers.append(nn.Dropout(0.5))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.down1 = conv_block(3, 64, norm=False)\n",
    "        self.down2 = conv_block(64, 128)\n",
    "        self.down3 = conv_block(128, 256)\n",
    "        self.down4 = conv_block(256, 512)\n",
    "        self.down5 = conv_block(512, 512)\n",
    "        self.down6 = conv_block(512, 512)\n",
    "        self.down7 = conv_block(512, 512)\n",
    "        self.down8 = conv_block(512, 512, norm=False)\n",
    "\n",
    "        self.up1 = deconv_block(512, 512, dropout=True)\n",
    "        self.up2 = deconv_block(1024, 512, dropout=True)\n",
    "        self.up3 = deconv_block(1024, 512, dropout=True)\n",
    "        self.up4 = deconv_block(1024, 512)\n",
    "        self.up5 = deconv_block(1024, 256)\n",
    "        self.up6 = deconv_block(512, 128)\n",
    "        self.up7 = deconv_block(256, 64)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8)\n",
    "        u2 = self.up2(torch.cat([u1, d7], 1))\n",
    "        u3 = self.up3(torch.cat([u2, d6], 1))\n",
    "        u4 = self.up4(torch.cat([u3, d5], 1))\n",
    "        u5 = self.up5(torch.cat([u4, d4], 1))\n",
    "        u6 = self.up6(torch.cat([u5, d3], 1))\n",
    "        u7 = self.up7(torch.cat([u6, d2], 1))\n",
    "        return self.final(torch.cat([u7, d1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "101ae393-b1d5-4ac0-9148-62cc85e9c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "#  Discriminator (PatchGAN)\n",
    "# =======================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c, norm=True):\n",
    "            layers = [nn.Conv2d(in_c, out_c, 4, 2, 1)]\n",
    "            if norm: layers.append(nn.InstanceNorm2d(out_c))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(3, 64, norm=False),\n",
    "            *block(64, 128),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd9b317-ff66-4e5e-bcfc-8c7b3bd3cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "#  Training Config\n",
    "# =======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G_AB = Generator().to(device)\n",
    "G_BA = Generator().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "optimizer_G = optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_A = optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "dataset = UnpairedDataset(\"Unziped AB\", transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d16cb328-60c8-490f-a03a-48b3c5447f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200]  Loss_G: 2.4285  Loss_D_A: 0.0104  Loss_D_B: 0.1729\n",
      "[Epoch 2/200]  Loss_G: 1.7141  Loss_D_A: 0.0325  Loss_D_B: 0.1756\n",
      "[Epoch 3/200]  Loss_G: 3.0671  Loss_D_A: 0.2035  Loss_D_B: 0.1117\n",
      "[Epoch 4/200]  Loss_G: 1.3522  Loss_D_A: 0.2052  Loss_D_B: 0.2354\n",
      "[Epoch 5/200]  Loss_G: 1.5879  Loss_D_A: 0.0430  Loss_D_B: 0.1527\n",
      "[Epoch 6/200]  Loss_G: 2.0615  Loss_D_A: 0.1114  Loss_D_B: 0.2629\n",
      "[Epoch 7/200]  Loss_G: 2.0243  Loss_D_A: 0.0139  Loss_D_B: 0.0320\n",
      "[Epoch 8/200]  Loss_G: 1.7645  Loss_D_A: 0.0213  Loss_D_B: 0.0273\n",
      "[Epoch 9/200]  Loss_G: 1.3541  Loss_D_A: 0.1359  Loss_D_B: 0.1547\n",
      "[Epoch 10/200]  Loss_G: 1.7471  Loss_D_A: 0.0095  Loss_D_B: 0.0537\n",
      "[Epoch 11/200]  Loss_G: 1.8559  Loss_D_A: 0.0220  Loss_D_B: 0.0349\n",
      "[Epoch 12/200]  Loss_G: 2.0516  Loss_D_A: 0.0536  Loss_D_B: 0.0188\n",
      "[Epoch 13/200]  Loss_G: 1.4028  Loss_D_A: 0.2125  Loss_D_B: 0.1191\n",
      "[Epoch 14/200]  Loss_G: 1.0737  Loss_D_A: 0.1976  Loss_D_B: 0.3981\n",
      "[Epoch 15/200]  Loss_G: 1.8100  Loss_D_A: 0.0412  Loss_D_B: 0.0101\n",
      "[Epoch 16/200]  Loss_G: 1.4585  Loss_D_A: 0.0625  Loss_D_B: 0.1563\n",
      "[Epoch 17/200]  Loss_G: 1.3344  Loss_D_A: 0.0155  Loss_D_B: 0.3984\n",
      "[Epoch 18/200]  Loss_G: 2.2853  Loss_D_A: 0.0209  Loss_D_B: 0.2483\n",
      "[Epoch 19/200]  Loss_G: 1.3318  Loss_D_A: 0.0366  Loss_D_B: 0.1503\n",
      "[Epoch 20/200]  Loss_G: 1.7369  Loss_D_A: 0.0785  Loss_D_B: 0.0473\n",
      "[Epoch 21/200]  Loss_G: 1.4460  Loss_D_A: 0.0922  Loss_D_B: 0.1177\n",
      "[Epoch 22/200]  Loss_G: 1.3431  Loss_D_A: 0.3921  Loss_D_B: 0.3625\n",
      "[Epoch 23/200]  Loss_G: 1.3782  Loss_D_A: 0.2303  Loss_D_B: 0.2167\n",
      "[Epoch 24/200]  Loss_G: 1.5479  Loss_D_A: 0.1572  Loss_D_B: 0.1479\n",
      "[Epoch 25/200]  Loss_G: 1.5740  Loss_D_A: 0.0311  Loss_D_B: 0.0460\n",
      "[Epoch 26/200]  Loss_G: 1.7555  Loss_D_A: 0.0437  Loss_D_B: 0.0926\n",
      "[Epoch 27/200]  Loss_G: 1.7086  Loss_D_A: 0.1383  Loss_D_B: 0.1825\n",
      "[Epoch 28/200]  Loss_G: 1.4278  Loss_D_A: 0.0703  Loss_D_B: 0.0748\n",
      "[Epoch 29/200]  Loss_G: 1.5632  Loss_D_A: 0.0386  Loss_D_B: 0.1007\n",
      "[Epoch 30/200]  Loss_G: 1.9957  Loss_D_A: 0.0252  Loss_D_B: 0.1242\n",
      "[Epoch 31/200]  Loss_G: 1.4828  Loss_D_A: 0.0197  Loss_D_B: 0.1493\n",
      "[Epoch 32/200]  Loss_G: 1.4439  Loss_D_A: 0.0463  Loss_D_B: 0.0948\n",
      "[Epoch 33/200]  Loss_G: 1.8972  Loss_D_A: 0.0863  Loss_D_B: 0.2126\n",
      "[Epoch 34/200]  Loss_G: 2.0838  Loss_D_A: 0.0335  Loss_D_B: 0.2060\n",
      "[Epoch 35/200]  Loss_G: 1.6201  Loss_D_A: 0.0276  Loss_D_B: 0.1102\n",
      "[Epoch 36/200]  Loss_G: 1.6586  Loss_D_A: 0.0928  Loss_D_B: 0.1027\n",
      "[Epoch 37/200]  Loss_G: 1.5805  Loss_D_A: 0.1858  Loss_D_B: 0.1473\n",
      "[Epoch 38/200]  Loss_G: 2.1202  Loss_D_A: 0.0492  Loss_D_B: 0.0730\n",
      "[Epoch 39/200]  Loss_G: 1.8436  Loss_D_A: 0.0172  Loss_D_B: 0.0487\n",
      "[Epoch 40/200]  Loss_G: 1.9942  Loss_D_A: 0.0861  Loss_D_B: 0.1047\n",
      "[Epoch 41/200]  Loss_G: 1.5484  Loss_D_A: 0.1064  Loss_D_B: 0.1052\n",
      "[Epoch 42/200]  Loss_G: 1.8671  Loss_D_A: 0.0363  Loss_D_B: 0.0614\n",
      "[Epoch 43/200]  Loss_G: 1.3958  Loss_D_A: 0.1238  Loss_D_B: 0.1527\n",
      "[Epoch 44/200]  Loss_G: 1.5009  Loss_D_A: 0.1154  Loss_D_B: 0.1621\n",
      "[Epoch 45/200]  Loss_G: 2.2457  Loss_D_A: 0.0998  Loss_D_B: 0.0852\n",
      "[Epoch 46/200]  Loss_G: 1.6058  Loss_D_A: 0.0685  Loss_D_B: 0.0955\n",
      "[Epoch 47/200]  Loss_G: 1.3390  Loss_D_A: 0.1604  Loss_D_B: 0.2487\n",
      "[Epoch 48/200]  Loss_G: 2.1620  Loss_D_A: 0.0640  Loss_D_B: 0.0829\n",
      "[Epoch 49/200]  Loss_G: 2.2413  Loss_D_A: 0.1057  Loss_D_B: 0.0332\n",
      "[Epoch 50/200]  Loss_G: 1.6163  Loss_D_A: 0.0594  Loss_D_B: 0.1544\n",
      "[Epoch 51/200]  Loss_G: 1.7336  Loss_D_A: 0.0417  Loss_D_B: 0.1217\n",
      "[Epoch 52/200]  Loss_G: 1.8896  Loss_D_A: 0.0684  Loss_D_B: 0.0648\n",
      "[Epoch 53/200]  Loss_G: 1.6007  Loss_D_A: 0.1503  Loss_D_B: 0.0478\n",
      "[Epoch 54/200]  Loss_G: 1.3787  Loss_D_A: 0.0572  Loss_D_B: 0.0778\n",
      "[Epoch 55/200]  Loss_G: 1.7204  Loss_D_A: 0.0208  Loss_D_B: 0.0363\n",
      "[Epoch 56/200]  Loss_G: 1.5683  Loss_D_A: 0.1176  Loss_D_B: 0.2527\n",
      "[Epoch 57/200]  Loss_G: 1.5809  Loss_D_A: 0.0848  Loss_D_B: 0.2294\n",
      "[Epoch 58/200]  Loss_G: 1.5433  Loss_D_A: 0.1618  Loss_D_B: 0.1454\n",
      "[Epoch 59/200]  Loss_G: 1.5316  Loss_D_A: 0.0554  Loss_D_B: 0.0591\n",
      "[Epoch 60/200]  Loss_G: 1.4690  Loss_D_A: 0.0911  Loss_D_B: 0.0643\n",
      "[Epoch 61/200]  Loss_G: 1.8881  Loss_D_A: 0.0211  Loss_D_B: 0.0728\n",
      "[Epoch 62/200]  Loss_G: 1.9248  Loss_D_A: 0.0812  Loss_D_B: 0.1302\n",
      "[Epoch 63/200]  Loss_G: 2.1131  Loss_D_A: 0.0331  Loss_D_B: 0.1146\n",
      "[Epoch 64/200]  Loss_G: 1.5307  Loss_D_A: 0.0326  Loss_D_B: 0.1979\n",
      "[Epoch 65/200]  Loss_G: 1.5823  Loss_D_A: 0.3275  Loss_D_B: 0.0865\n",
      "[Epoch 66/200]  Loss_G: 1.4350  Loss_D_A: 0.1405  Loss_D_B: 0.1290\n",
      "[Epoch 67/200]  Loss_G: 1.7108  Loss_D_A: 0.2106  Loss_D_B: 0.1483\n",
      "[Epoch 68/200]  Loss_G: 1.7213  Loss_D_A: 0.0809  Loss_D_B: 0.0742\n",
      "[Epoch 69/200]  Loss_G: 1.4239  Loss_D_A: 0.3242  Loss_D_B: 0.1650\n",
      "[Epoch 70/200]  Loss_G: 1.9291  Loss_D_A: 0.1003  Loss_D_B: 0.0954\n",
      "[Epoch 71/200]  Loss_G: 1.4565  Loss_D_A: 0.0894  Loss_D_B: 0.1197\n",
      "[Epoch 72/200]  Loss_G: 2.2125  Loss_D_A: 0.0851  Loss_D_B: 0.0914\n",
      "[Epoch 73/200]  Loss_G: 1.9594  Loss_D_A: 0.1757  Loss_D_B: 0.0600\n",
      "[Epoch 74/200]  Loss_G: 1.6339  Loss_D_A: 0.0836  Loss_D_B: 0.1694\n",
      "[Epoch 75/200]  Loss_G: 1.7309  Loss_D_A: 0.0352  Loss_D_B: 0.0459\n",
      "[Epoch 76/200]  Loss_G: 1.6219  Loss_D_A: 0.0382  Loss_D_B: 0.0586\n",
      "[Epoch 77/200]  Loss_G: 1.5629  Loss_D_A: 0.1290  Loss_D_B: 0.0525\n",
      "[Epoch 78/200]  Loss_G: 2.0680  Loss_D_A: 0.0243  Loss_D_B: 0.0907\n",
      "[Epoch 79/200]  Loss_G: 2.2731  Loss_D_A: 0.1139  Loss_D_B: 0.1101\n",
      "[Epoch 80/200]  Loss_G: 1.2184  Loss_D_A: 0.1069  Loss_D_B: 0.1088\n",
      "[Epoch 81/200]  Loss_G: 1.2951  Loss_D_A: 0.0715  Loss_D_B: 0.1419\n",
      "[Epoch 82/200]  Loss_G: 1.8216  Loss_D_A: 0.1360  Loss_D_B: 0.1008\n",
      "[Epoch 83/200]  Loss_G: 1.3900  Loss_D_A: 0.1962  Loss_D_B: 0.0629\n",
      "[Epoch 84/200]  Loss_G: 1.8110  Loss_D_A: 0.0385  Loss_D_B: 0.0484\n",
      "[Epoch 85/200]  Loss_G: 1.5066  Loss_D_A: 0.1171  Loss_D_B: 0.0355\n",
      "[Epoch 86/200]  Loss_G: 2.0128  Loss_D_A: 0.1479  Loss_D_B: 0.0292\n",
      "[Epoch 87/200]  Loss_G: 1.5538  Loss_D_A: 0.0747  Loss_D_B: 0.0331\n",
      "[Epoch 88/200]  Loss_G: 1.7184  Loss_D_A: 0.0990  Loss_D_B: 0.1304\n",
      "[Epoch 89/200]  Loss_G: 1.5568  Loss_D_A: 0.0743  Loss_D_B: 0.0330\n",
      "[Epoch 90/200]  Loss_G: 1.5634  Loss_D_A: 0.0528  Loss_D_B: 0.0193\n",
      "[Epoch 91/200]  Loss_G: 1.5714  Loss_D_A: 0.0548  Loss_D_B: 0.0404\n",
      "[Epoch 92/200]  Loss_G: 1.7255  Loss_D_A: 0.1374  Loss_D_B: 0.0497\n",
      "[Epoch 93/200]  Loss_G: 1.6752  Loss_D_A: 0.1172  Loss_D_B: 0.0529\n",
      "[Epoch 94/200]  Loss_G: 1.7824  Loss_D_A: 0.0353  Loss_D_B: 0.0368\n",
      "[Epoch 95/200]  Loss_G: 2.5018  Loss_D_A: 0.0542  Loss_D_B: 0.0238\n",
      "[Epoch 96/200]  Loss_G: 1.5970  Loss_D_A: 0.0316  Loss_D_B: 0.0533\n",
      "[Epoch 97/200]  Loss_G: 1.6959  Loss_D_A: 0.1116  Loss_D_B: 0.0262\n",
      "[Epoch 98/200]  Loss_G: 1.6188  Loss_D_A: 0.0852  Loss_D_B: 0.0169\n",
      "[Epoch 99/200]  Loss_G: 1.6816  Loss_D_A: 0.0454  Loss_D_B: 0.0520\n",
      "[Epoch 100/200]  Loss_G: 1.5689  Loss_D_A: 0.0574  Loss_D_B: 0.0161\n",
      "[Epoch 101/200]  Loss_G: 2.0156  Loss_D_A: 0.1052  Loss_D_B: 0.0263\n",
      "[Epoch 102/200]  Loss_G: 1.4747  Loss_D_A: 0.1052  Loss_D_B: 0.0444\n",
      "[Epoch 103/200]  Loss_G: 1.8765  Loss_D_A: 0.0699  Loss_D_B: 0.0809\n",
      "[Epoch 104/200]  Loss_G: 2.0032  Loss_D_A: 0.1424  Loss_D_B: 0.0465\n",
      "[Epoch 105/200]  Loss_G: 1.4135  Loss_D_A: 0.0948  Loss_D_B: 0.0142\n",
      "[Epoch 106/200]  Loss_G: 1.8516  Loss_D_A: 0.0779  Loss_D_B: 0.0397\n",
      "[Epoch 107/200]  Loss_G: 1.4116  Loss_D_A: 0.1032  Loss_D_B: 0.0485\n",
      "[Epoch 108/200]  Loss_G: 1.7371  Loss_D_A: 0.0497  Loss_D_B: 0.0262\n",
      "[Epoch 109/200]  Loss_G: 1.9082  Loss_D_A: 0.0968  Loss_D_B: 0.1076\n",
      "[Epoch 110/200]  Loss_G: 1.6755  Loss_D_A: 0.2521  Loss_D_B: 0.0369\n",
      "[Epoch 111/200]  Loss_G: 1.8592  Loss_D_A: 0.1321  Loss_D_B: 0.0754\n",
      "[Epoch 112/200]  Loss_G: 1.4794  Loss_D_A: 0.0553  Loss_D_B: 0.0622\n",
      "[Epoch 113/200]  Loss_G: 1.9683  Loss_D_A: 0.1313  Loss_D_B: 0.0265\n",
      "[Epoch 114/200]  Loss_G: 1.8761  Loss_D_A: 0.1164  Loss_D_B: 0.0178\n",
      "[Epoch 115/200]  Loss_G: 1.7254  Loss_D_A: 0.0669  Loss_D_B: 0.0228\n",
      "[Epoch 116/200]  Loss_G: 1.7781  Loss_D_A: 0.1134  Loss_D_B: 0.0170\n",
      "[Epoch 117/200]  Loss_G: 1.7255  Loss_D_A: 0.1577  Loss_D_B: 0.0154\n",
      "[Epoch 118/200]  Loss_G: 1.6705  Loss_D_A: 0.0913  Loss_D_B: 0.0780\n",
      "[Epoch 119/200]  Loss_G: 1.9731  Loss_D_A: 0.0818  Loss_D_B: 0.0761\n",
      "[Epoch 120/200]  Loss_G: 1.6479  Loss_D_A: 0.1410  Loss_D_B: 0.0842\n",
      "[Epoch 121/200]  Loss_G: 2.1639  Loss_D_A: 0.1140  Loss_D_B: 0.0344\n",
      "[Epoch 122/200]  Loss_G: 1.6042  Loss_D_A: 0.0823  Loss_D_B: 0.0232\n",
      "[Epoch 123/200]  Loss_G: 1.5758  Loss_D_A: 0.0787  Loss_D_B: 0.0097\n",
      "[Epoch 124/200]  Loss_G: 1.6072  Loss_D_A: 0.0397  Loss_D_B: 0.0198\n",
      "[Epoch 125/200]  Loss_G: 1.5172  Loss_D_A: 0.1014  Loss_D_B: 0.0258\n",
      "[Epoch 126/200]  Loss_G: 1.6010  Loss_D_A: 0.1516  Loss_D_B: 0.0138\n",
      "[Epoch 127/200]  Loss_G: 1.7383  Loss_D_A: 0.1578  Loss_D_B: 0.0275\n",
      "[Epoch 128/200]  Loss_G: 1.5869  Loss_D_A: 0.1340  Loss_D_B: 0.0296\n",
      "[Epoch 129/200]  Loss_G: 1.6295  Loss_D_A: 0.0798  Loss_D_B: 0.0183\n",
      "[Epoch 130/200]  Loss_G: 1.7479  Loss_D_A: 0.0573  Loss_D_B: 0.0233\n",
      "[Epoch 131/200]  Loss_G: 1.6014  Loss_D_A: 0.1810  Loss_D_B: 0.0226\n",
      "[Epoch 132/200]  Loss_G: 1.7001  Loss_D_A: 0.0454  Loss_D_B: 0.0225\n",
      "[Epoch 133/200]  Loss_G: 1.6089  Loss_D_A: 0.1205  Loss_D_B: 0.0723\n",
      "[Epoch 134/200]  Loss_G: 1.8058  Loss_D_A: 0.0741  Loss_D_B: 0.0141\n",
      "[Epoch 135/200]  Loss_G: 1.3054  Loss_D_A: 0.1315  Loss_D_B: 0.0950\n",
      "[Epoch 136/200]  Loss_G: 1.4700  Loss_D_A: 0.0792  Loss_D_B: 0.0471\n",
      "[Epoch 137/200]  Loss_G: 2.5233  Loss_D_A: 0.0533  Loss_D_B: 0.0288\n",
      "[Epoch 138/200]  Loss_G: 1.8338  Loss_D_A: 0.2297  Loss_D_B: 0.0167\n",
      "[Epoch 139/200]  Loss_G: 1.6227  Loss_D_A: 0.2097  Loss_D_B: 0.0155\n",
      "[Epoch 140/200]  Loss_G: 1.7366  Loss_D_A: 0.0427  Loss_D_B: 0.0430\n",
      "[Epoch 141/200]  Loss_G: 1.7037  Loss_D_A: 0.1889  Loss_D_B: 0.0221\n",
      "[Epoch 142/200]  Loss_G: 1.7692  Loss_D_A: 0.1272  Loss_D_B: 0.0575\n",
      "[Epoch 143/200]  Loss_G: 1.7582  Loss_D_A: 0.1238  Loss_D_B: 0.0525\n",
      "[Epoch 144/200]  Loss_G: 1.9700  Loss_D_A: 0.0932  Loss_D_B: 0.0121\n",
      "[Epoch 145/200]  Loss_G: 1.5748  Loss_D_A: 0.1010  Loss_D_B: 0.0721\n",
      "[Epoch 146/200]  Loss_G: 1.8523  Loss_D_A: 0.0649  Loss_D_B: 0.0328\n",
      "[Epoch 147/200]  Loss_G: 1.4365  Loss_D_A: 0.0830  Loss_D_B: 0.0290\n",
      "[Epoch 148/200]  Loss_G: 1.7675  Loss_D_A: 0.0390  Loss_D_B: 0.0231\n",
      "[Epoch 149/200]  Loss_G: 1.6183  Loss_D_A: 0.1392  Loss_D_B: 0.0175\n",
      "[Epoch 150/200]  Loss_G: 1.5598  Loss_D_A: 0.0421  Loss_D_B: 0.1459\n",
      "[Epoch 151/200]  Loss_G: 1.5236  Loss_D_A: 0.0891  Loss_D_B: 0.1094\n",
      "[Epoch 152/200]  Loss_G: 1.7748  Loss_D_A: 0.0520  Loss_D_B: 0.0127\n",
      "[Epoch 153/200]  Loss_G: 2.0906  Loss_D_A: 0.1424  Loss_D_B: 0.0398\n",
      "[Epoch 154/200]  Loss_G: 1.4963  Loss_D_A: 0.2031  Loss_D_B: 0.0515\n",
      "[Epoch 155/200]  Loss_G: 1.9256  Loss_D_A: 0.1116  Loss_D_B: 0.0595\n",
      "[Epoch 156/200]  Loss_G: 1.2520  Loss_D_A: 0.1475  Loss_D_B: 0.0598\n",
      "[Epoch 157/200]  Loss_G: 1.6049  Loss_D_A: 0.0786  Loss_D_B: 0.0115\n",
      "[Epoch 158/200]  Loss_G: 1.9905  Loss_D_A: 0.0655  Loss_D_B: 0.0457\n",
      "[Epoch 159/200]  Loss_G: 1.8352  Loss_D_A: 0.0527  Loss_D_B: 0.0237\n",
      "[Epoch 160/200]  Loss_G: 2.0384  Loss_D_A: 0.0996  Loss_D_B: 0.0337\n",
      "[Epoch 161/200]  Loss_G: 1.3136  Loss_D_A: 0.0974  Loss_D_B: 0.0200\n",
      "[Epoch 162/200]  Loss_G: 1.7935  Loss_D_A: 0.1189  Loss_D_B: 0.0340\n",
      "[Epoch 163/200]  Loss_G: 1.8026  Loss_D_A: 0.0947  Loss_D_B: 0.0160\n",
      "[Epoch 164/200]  Loss_G: 1.9893  Loss_D_A: 0.0887  Loss_D_B: 0.0290\n",
      "[Epoch 165/200]  Loss_G: 1.6408  Loss_D_A: 0.0404  Loss_D_B: 0.0189\n",
      "[Epoch 166/200]  Loss_G: 1.5290  Loss_D_A: 0.0704  Loss_D_B: 0.0212\n",
      "[Epoch 167/200]  Loss_G: 2.0076  Loss_D_A: 0.1225  Loss_D_B: 0.0291\n",
      "[Epoch 168/200]  Loss_G: 1.7250  Loss_D_A: 0.0467  Loss_D_B: 0.0255\n",
      "[Epoch 169/200]  Loss_G: 1.7511  Loss_D_A: 0.0922  Loss_D_B: 0.1636\n",
      "[Epoch 170/200]  Loss_G: 1.9380  Loss_D_A: 0.0731  Loss_D_B: 0.0570\n",
      "[Epoch 171/200]  Loss_G: 1.6189  Loss_D_A: 0.1022  Loss_D_B: 0.0724\n",
      "[Epoch 172/200]  Loss_G: 1.8826  Loss_D_A: 0.1300  Loss_D_B: 0.0223\n",
      "[Epoch 173/200]  Loss_G: 1.4802  Loss_D_A: 0.1050  Loss_D_B: 0.0543\n",
      "[Epoch 174/200]  Loss_G: 1.2750  Loss_D_A: 0.1221  Loss_D_B: 0.0261\n",
      "[Epoch 175/200]  Loss_G: 1.9360  Loss_D_A: 0.1130  Loss_D_B: 0.0357\n",
      "[Epoch 176/200]  Loss_G: 2.2377  Loss_D_A: 0.1011  Loss_D_B: 0.0775\n",
      "[Epoch 177/200]  Loss_G: 1.8680  Loss_D_A: 0.0813  Loss_D_B: 0.0102\n",
      "[Epoch 178/200]  Loss_G: 1.5002  Loss_D_A: 0.0634  Loss_D_B: 0.0251\n",
      "[Epoch 179/200]  Loss_G: 1.5970  Loss_D_A: 0.0887  Loss_D_B: 0.0186\n",
      "[Epoch 180/200]  Loss_G: 1.6719  Loss_D_A: 0.1309  Loss_D_B: 0.0301\n",
      "[Epoch 181/200]  Loss_G: 1.5423  Loss_D_A: 0.0926  Loss_D_B: 0.0520\n",
      "[Epoch 182/200]  Loss_G: 1.8718  Loss_D_A: 0.1797  Loss_D_B: 0.0192\n",
      "[Epoch 183/200]  Loss_G: 1.7672  Loss_D_A: 0.0778  Loss_D_B: 0.0256\n",
      "[Epoch 184/200]  Loss_G: 2.0122  Loss_D_A: 0.0805  Loss_D_B: 0.0427\n",
      "[Epoch 185/200]  Loss_G: 1.8396  Loss_D_A: 0.1544  Loss_D_B: 0.0613\n",
      "[Epoch 186/200]  Loss_G: 1.6695  Loss_D_A: 0.0355  Loss_D_B: 0.0513\n",
      "[Epoch 187/200]  Loss_G: 1.8146  Loss_D_A: 0.1575  Loss_D_B: 0.0255\n",
      "[Epoch 188/200]  Loss_G: 1.9368  Loss_D_A: 0.0765  Loss_D_B: 0.0153\n",
      "[Epoch 189/200]  Loss_G: 1.4929  Loss_D_A: 0.1366  Loss_D_B: 0.0753\n",
      "[Epoch 190/200]  Loss_G: 2.2977  Loss_D_A: 0.0507  Loss_D_B: 0.0383\n",
      "[Epoch 191/200]  Loss_G: 1.9453  Loss_D_A: 0.0903  Loss_D_B: 0.0366\n",
      "[Epoch 192/200]  Loss_G: 1.6705  Loss_D_A: 0.0773  Loss_D_B: 0.0416\n",
      "[Epoch 193/200]  Loss_G: 2.0014  Loss_D_A: 0.1191  Loss_D_B: 0.0896\n",
      "[Epoch 194/200]  Loss_G: 1.7843  Loss_D_A: 0.1333  Loss_D_B: 0.0617\n",
      "[Epoch 195/200]  Loss_G: 2.0489  Loss_D_A: 0.0598  Loss_D_B: 0.0378\n",
      "[Epoch 196/200]  Loss_G: 1.6392  Loss_D_A: 0.2219  Loss_D_B: 0.1793\n",
      "[Epoch 197/200]  Loss_G: 1.4377  Loss_D_A: 0.1716  Loss_D_B: 0.0546\n",
      "[Epoch 198/200]  Loss_G: 1.8849  Loss_D_A: 0.2471  Loss_D_B: 0.0333\n",
      "[Epoch 199/200]  Loss_G: 1.6384  Loss_D_A: 0.1038  Loss_D_B: 0.0711\n",
      "[Epoch 200/200]  Loss_G: 1.8085  Loss_D_A: 0.0497  Loss_D_B: 0.1111\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "#  Training Loop\n",
    "# =======================\n",
    "epochs = 200\n",
    "lambda_cycle = 10\n",
    "lambda_id = 5\n",
    "\n",
    "os.makedirs(\"outputsAtoB\", exist_ok=True)\n",
    "os.makedirs(\"outputsBtoA\", exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_A, real_B) in enumerate(loader):\n",
    "        real_A, real_B = real_A.to(device), real_B.to(device)\n",
    "\n",
    "        # Adjust the size of valid and fake tensors to match discriminator output\n",
    "        valid = torch.ones((real_A.size(0), 1, 15, 15), device=device)\n",
    "        fake = torch.zeros((real_A.size(0), 1, 15, 15), device=device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generators\n",
    "        # ---------------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_B = G_AB(real_A)\n",
    "        fake_A = G_BA(real_B)\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "        loss_identity = (loss_id_A + loss_id_B) * lambda_id * 0.5\n",
    "\n",
    "        # GAN loss\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "        loss_GAN_total = (loss_GAN_AB + loss_GAN_BA) * 0.5\n",
    "\n",
    "        # Cycle consistency\n",
    "        recov_A = G_BA(fake_B)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) * lambda_cycle\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = loss_GAN_total + loss_cycle + loss_identity\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminators\n",
    "        # ---------------------\n",
    "        optimizer_D_A.zero_grad()\n",
    "        loss_real_A = criterion_GAN(D_A(real_A), valid)\n",
    "        loss_fake_A = criterion_GAN(D_A(fake_A.detach()), fake)\n",
    "        loss_D_A = (loss_real_A + loss_fake_A) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "        loss_real_B = criterion_GAN(D_B(real_B), valid)\n",
    "        loss_fake_B = criterion_GAN(D_B(fake_B.detach()), fake)\n",
    "        loss_D_B = (loss_real_B + loss_fake_B) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}]  Loss_G: {loss_G.item():.4f}  Loss_D_A: {loss_D_A.item():.4f}  Loss_D_B: {loss_D_B.item():.4f}\")\n",
    "\n",
    "    # Save sample images\n",
    "    save_image((fake_B + 1) / 2, f\"outputsAtoB/fakeB_epoch_{epoch+1}.png\")\n",
    "    save_image((fake_A + 1) / 2, f\"outputsBtoA/fakeA_epoch_{epoch+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df1c7c10-43f4-49e3-9fcd-41af6654d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained generators\n",
    "torch.save(G_AB.state_dict(), \"G_A2B_trained.pth\")\n",
    "\n",
    "torch.save(G_BA.state_dict(), \"G_B2A_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d57d391-15ad-4947-89c4-8492f613b300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9624\\932325315.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G_AB.load_state_dict(torch.load(r\"G_A2B_trained.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¨ Ghibli-style image saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model\n",
    "G_AB = Generator()  # your generator class\n",
    "G_AB.load_state_dict(torch.load(r\"G_A2B_trained.pth\", map_location=device))\n",
    "G_AB.to(device)\n",
    "G_AB.eval()\n",
    "\n",
    "# Load and preprocess user image\n",
    "input_image = Image.open(\"33.jpg\").convert(\"RGB\")\n",
    "input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate Ghibli-style image\n",
    "with torch.no_grad():\n",
    "    fake_B = G_AB(input_tensor)\n",
    "\n",
    "# Denormalize and save\n",
    "fake_B = (fake_B + 1) / 2\n",
    "save_image(fake_B.cpu(), \"Output1.png\")\n",
    "\n",
    "print(\"ðŸŽ¨ Ghibli-style image saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9528d-2c21-432b-95c4-42283ddc0118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
